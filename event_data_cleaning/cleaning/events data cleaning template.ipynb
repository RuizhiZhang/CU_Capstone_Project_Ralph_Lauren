{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CITY_NAME = 'LAS VEGAS'\n",
    "STORE_NAME = 'LAS VEGAS SOUTH'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales/traffic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge (if not done before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_clean = pd.read_csv('data/1208_LasVegas_South_sales_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_clean.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_clean.date = pd.to_datetime(sales_clean.date, format='%m/%d/%y')\n",
    "# sales_clean = sales_clean[['date', 'sales_original', 'sales_cleaned']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traffic_clean = pd.read_csv('data/1208_LasVegas_South_traffic_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traffic_clean.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traffic_clean.date = pd.to_datetime(traffic_clean.date, format='%m/%d/%y')\n",
    "# traffic_clean = traffic_clean[['date', 'traffic_original', 'traffic_cleaned']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert sales_clean.shape[0] == traffic_clean.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_traffic_clean = pd.merge(sales_clean, traffic_clean, on='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_traffic_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data (if merged before)\n",
    "- Assume the sales/traffic data have the format: `'sales_traffic_' + STORE_NAME + '.csv'`\n",
    "- With columns `date`, `sales_original`, `sales_cleaned`, `traffic_original`, `traffic_cleaned`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_traffic_clean = pd.read_csv('sales_traffic_' + STORE_NAME + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_traffic_clean.date = pd.to_datetime(sales_traffic_clean.date, format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant date values\n",
    "DATE_MIN = sales_traffic_clean.date.min().normalize()\n",
    "DATE_MAX = sales_traffic_clean.date.max().normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE_MIN, DATE_MAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Events Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assume the events data have the format: `'events_' + CITY_NAME + '.csv'`\n",
    "- With the raw columns from PredictHQ plus the generated `venue_type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant location for stores\n",
    "LAT_STORE, LON_STORE = {}, {}\n",
    "\n",
    "LAT_STORE['ORLANDO FOA'] = 28.473595\n",
    "LAT_STORE['LAKE BUENA VISTA FOA'] = 28.387852\n",
    "LAT_STORE['LANCASTER FSC'] = 40.025636\n",
    "LAT_STORE['LAS VEGAS NORTH'] = 36.170727\n",
    "LAT_STORE['LAS VEGAS SOUTH'] = 36.056725\n",
    "\n",
    "LON_STORE['ORLANDO FOA'] = -81.451615\n",
    "LON_STORE['LAKE BUENA VISTA FOA'] = -81.493674\n",
    "LON_STORE['LANCASTER FSC'] = -76.217167\n",
    "LON_STORE['LAS VEGAS NORTH'] = -115.157651\n",
    "LON_STORE['LAS VEGAS SOUTH'] = -115.170121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = pd.read_csv('events_' + CITY_NAME + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpu\n",
    "\n",
    "def draw_events_data(events, store_name, date_min=DATE_MIN, date_max=DATE_MAX, \n",
    "                     lat_store=LAT_STORE, lon_store=LON_STORE):\n",
    "    \n",
    "    def compute_time(df):\n",
    "        if df['start'].hour >= 12:\n",
    "            return 'pm'\n",
    "        else:\n",
    "            return 'am'\n",
    "    \n",
    "    def compute_dist(df, store_name):\n",
    "        return mpu.haversine_distance((df.latitude, df.longitude), \n",
    "                                      (lat_store[store_name], lon_store[store_name]))\n",
    "    \n",
    "    def compute_capacity(df):\n",
    "        # exp((rank + 19.769) / 9.6442) by reverse engineering\n",
    "        return np.exp((df['rank'] + 19.769) / 9.6442)\n",
    "    \n",
    "    def compute_impact(df, method):\n",
    "        if method == 'linear':\n",
    "            return df['rank'] / df['distance']\n",
    "        if method == 'exp':\n",
    "            return np.sqrt(np.exp(df['rank'])) / np.square(df['distance'])\n",
    "    \n",
    "    # Split location variable into longitude and latitude\n",
    "    lat = [float(events.location[i].strip(\"\\\"\").split(',')[0]) \\\n",
    "           for i in range(len(events))]\n",
    "    lon = [float(events.location[i].strip(\"\\\"\").split(',')[1]) \\\n",
    "           for i in range(len(events))]\n",
    "    events['longitude'] = lon\n",
    "    events['latitude'] = lat\n",
    "    \n",
    "    events.start = pd.to_datetime(events.start)\n",
    "    events.end = pd.to_datetime(events.end)\n",
    "    \n",
    "    # Extract time (am/pm)\n",
    "    events['start_time'] = events.apply(compute_time, axis=1)\n",
    "    \n",
    "    # Generate a date range for the events\n",
    "    events['start'] = events.start.dt.tz_localize(None).dt.normalize()\n",
    "    events['end'] = events.end.dt.tz_localize(None).dt.normalize()\n",
    "    \n",
    "    date_range = events.apply(lambda x: pd.date_range(x.start, x.end).tolist(), \n",
    "                              axis=1)\n",
    "    \n",
    "    events = events.drop(['id', 'start', 'end', 'predicted_end', \\\n",
    "                          'timezone', 'country', 'location', \\\n",
    "                          'venue_formatted_address', 'state', 'first_seen'], 1)\n",
    "    \n",
    "    events_by_date = pd.DataFrame(columns = events.columns)\n",
    "    dates_rearr = []\n",
    "    for i in range(len(events)):\n",
    "        for dates in date_range[i]:\n",
    "            if (dates <= date_max) & (dates >= date_min):\n",
    "                dates_rearr.append(dates)\n",
    "                events_by_date = events_by_date.append(events.iloc[i])\n",
    "    \n",
    "    # Aggregate new features\n",
    "    # Distance in miles\n",
    "    events_by_date['distance'] = events_by_date.apply(compute_dist, \n",
    "                                                  store_name=store_name, axis=1).div(1.609)\n",
    "    # Estimated capacity\n",
    "    events_by_date['est_capacity'] = events_by_date.apply(compute_capacity, axis=1)\n",
    "    # Impact\n",
    "    events_by_date['impact_linear'] = events_by_date.apply(compute_impact, method='linear', axis=1)\n",
    "#     events_by_date['impact_exp'] = events_by_date.apply(compute_impact, method='exp', axis=1)\n",
    "    \n",
    "    events_by_date['date'] = dates_rearr\n",
    "    # Check if an event is annual\n",
    "    events_by_date['year'] = events_by_date.date.dt.year\n",
    "    is_annual = events_by_date.groupby('title')['year'].nunique().to_frame('is_annual').reset_index()\n",
    "    is_annual['is_annual'] = is_annual.eval('is_annual == 3').astype(int)\n",
    "    events_by_date = pd.merge(events_by_date, is_annual, on='title')\n",
    "    \n",
    "    # Reorganize the columns\n",
    "    cols = ['title', 'description', 'labels', 'category', \\\n",
    "            'date', 'year', 'is_annual', 'start_time', 'duration', \\\n",
    "            'venue_name', 'scope', 'venue_type', 'est_capacity', 'distance', 'longitude', 'latitude', \\\n",
    "            'rank', 'local_rank', 'aviation_rank', 'impact_linear'] # 'impact_exp'\n",
    "    events_by_date = events_by_date[cols]\n",
    "    \n",
    "    # For efficiency, write out the data\n",
    "    events_by_date.to_csv('events_' + store_name + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_events_data(events, STORE_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
